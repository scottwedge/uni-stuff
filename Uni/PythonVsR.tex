\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{mathtools}
\usepackage{color}
\usepackage{hyperref}
\hypersetup{urlbordercolor={1 1 1}}


\begin{document}
The Structure of the thesis
\begin{enumerate}
    \item[1.] Motivation
        \begin{enumerate}
            \item[1.1] Python language overview
            \item[1.2] R language overview
            \item[1.3] Python vs R wars
            \item[1.4] Thesis description
            \item[1.5] Data preparation 
        \end{enumerate}
    \item[2.] Python
        \begin{enumerate}
            \item[2.1] The structure of the program
            \item[2.2] The process description
            \begin{enumerate}
                \item[2.2.1] Used Libraries
                \item[2.2.2] Documentation quality and quantity
                \item[2.2.3] Advantages and disadvantages of the program 
            \end{enumerate}
            \item[2.3] Results
        \end{enumerate}
    \item[3.] R
        \begin{enumerate}
            \item[3.1] The structure of the program
            \item[3.2] The process description
            \begin{enumerate}
                \item[3.2.1] Used Libraries
                \item[3.2.2] Documentation quality and quantity
                \item[3.2.3] Advantages and disadvantages of the program 
            \end{enumerate}
            \item[3.3] Results
        \end{enumerate}
    \item[4.] Results
        \begin{enumerate}
            \item[1.1] Objective comparison
            \item[1.2] Subjective comparison
            \item[1.3] Conclusion
        \end{enumerate}
    \item[5.] Literature
\end{enumerate}

\newpage
\section{Motivation}
Every time requires it's own heroes. Nowadays, with a massive digitalization of the world, statistical analysis is not only used in spheres like classical production and distribution (logistics), but also everywhere on the web. In order to create a nice web-site, that will attract buyers and more important will make them purchase things from a particular on-line shop using this exact site, on-line shops require an analysis of a customer's behavior, specific demands and preferences. Another example where data analysis is essential is video games industry. The AI, which is built in an immense data source and is constantly analyzing the incoming data from the players, servers and third party sources (if needed), is used in many game-projects from a simple browser-strategy to "the most expected game of the year". The game play, the enemies, the load distribution on the servers and many other things are based on analysis and forecast. The are many-many other examples where data analysis is used.\vspace{3mm}\\
Since the computer technologies are way more advanced, than they were several decades ago, the computations can be automatized, the data collection can be automatized. For every purposes certain tools, languages and libraries should be chosen carefully among the great choice. In this work we will concentrate our attention on a statistic-econometric task. On the whole, almost every language can be used to build a program for collecting, processing and visualizing data: SCALA, C, C++, .NET, Java, Python, JavaScript, R, etc.. However, some of the languages are more suitable for this purposes.\vspace{3mm}\\
In this work we will concentrate on "easy" languages. We will compare Python 3 (Python 2 in form of some libraries) language with R language. Both of them are relatively new (Python is about 25 years old and R is about 22 years old) to the wide publicity. Both are currently used for data analysis (for Python it is not the only use, but as we said before, we will concentrate on this aspect). Both languages are widely used in open source projects and have a large community behind them. Also, very important that both of the languages are relatively easy to begin with also for non-programmers. Python has a gradual learning curve due to its simplicity, clear and intuitive syntax. Although R has a steep learning curve it is still pretty easy to use for small and not too complex projects (which is the case for this thesis).

\newpage
\subsection{Python language overview}
The Python language is pretty young. It has first appeared about 25 years ago. As described on the official page in Wikipedia:"Python is a general-purpose, high-level programming language"[1]. That means that Python can be used for many tasks like back end development, front end development, data analysis and even for processor simulation (the python code will generate you VHDL code eventually). The language supports multiple programming paradigms: Object Oriented, Imperative and Functional programming.\\
Another big advantage of the language is being a cross-platform language executable via interpreter that can be installed on every operation system. Another possibility to run python programs without installing an interpreter is to package the program into stand-alone executable.\\ 
The language has an automatic garbage collector, so you don't have to worry about memory leaks by small to medium programs.\\
Enormous community stays behind Python and its \textcolor{blue}{variations}. The standard library has more than enough functions for easy start. The language's syntax is very clear and intuitive in comparison to C++, C or even Java. I would say the language is similar to normal human speech but in more strict and logical form.\\
Another good thing about Python is build-in test - doctest module. This test is very easy to use to check and debug your code on the fly. The use of the module is intuitively clear even to beginners. You also don't keep the test's text in other file, which gives you certain level of an overview of the program. Unit test are also widely used among the programmers using Python language. There also other possibilities to debug and benchmark the code written in python, but we will not discuss it here.\\
The disadvantage of the language is its speed in comparison with C, C++ and Java. Also the visualization tools could be better, but since the language is a multipurpose language, it is normal, that some things are not top among the class. \\

\subsection{R language overview}
According to Wikipedia:"R is a programming language and software environment for statistical computing and graphics"[5]. R is a successor of the S language, purposed about 39 years ago. R is distributed under GNU General Public license, which makes this language a nice choice for an open source analytical project.\\
Same as Python, R also uses interpreter to execute the code. R language is pretty much single-purpose language. The main use of R is statistical analysis and visualization. Since R is an open source project, there is a huge \textcolor{blue}{community of real life practice and scientists behind it}.\\
The disadvantage of the language is its one-purpose nature. Analytical project built with R will work with no doubt very fast and good, presenting good results. But the use of such project will be reduced, since there are very few quick and comfortable ways to integrate the program into the system.\\

\subsection{Python vs R wars}
Since both languages offer tons of packages and libraries for analyzing and visualization, people argue about what language is a better fit. You can find a lot of discussions about this topic on stackoverflow [6], stackoverflow analog for data science [7] and many other resources on internet [8],[9],[10].
So far nobody has managed to give a satisfactory final answer, since the languages indeed are different. However for every concrete task and scale we can give a list of requirements and compare the languages objectively leaving the subjective comparison aside.\\
Tho formal criteria to compare Python and R are:
\begin{enumerate}
    \item The amount of useful resources for the task
    \item Clear documentation with examples
    \item Performance
    \item Memory use
    \item Appropriate data structures
    \item Possibility to work with Big Data
    \item Visualization tools
    \item Hard or soft limitations
    \item Need of workarounds
\end{enumerate}
Using this list both languages can be objectively evaluated. These points are appropriate for all small-, middle- and big-projects. In next section we will describe the test-task to compare Python and R languages and define the points-system for objective evaluation.\\

\subsection{Thesis description}
The idea of this bachelor thesis is to test which language suits better specific statistic task. 
The program that will be evaluated consists of four parts:
\begin{enumerate}
    \item Getting and formatting data - the program should be able to read a prepared file in csv format. Afterwards, the program should format the data into needed structure for further use.
    \item Analyzing data - the program should be able to run statistical test to figure out data characteristics. This step is needed to determine what regression models are allowed for this data set. Following test will be presented: stationary test, build cdf and kde, find moments, distribution test (goodness of fit tests).
    \item Building a model - the program should be able to create a proper model using a step-forward algorithm, set the limits on the number of the predictors and return the names of the parameters, regression parameters and some useful statistics.
    \item Visualization - the program should be able to present the results and steps between if needed.
\end{enumerate}
The program will build a simple regression if possible. This task is considered to be middle-size task, not too easy but at the same time not too time and knowledge consuming.
The data will be prepared and provided as 2 files (training set and test set) in .csv format.\\
According to the list for objective evaluation, each point will give either 0 or 1 score to language. The 0 score will be given in negative case and score 1 will be given in positive case.
\begin{enumerate}
    \item[] Useful resources - at least 2 different libraries for one task gives 1 point.
    \item[] Documentation - examples, clean source code and clear structured APIs give 1 point.
    \item[] Performance - fastest language gets 1 point
    \item[] Memory - the program with the smallest memory use gets 1 point.
    \item[] Data structures - no additional formatting needed gives 1 point.
    \item[] Big Data - libraries, plugins and frameworks for big data give 1 point.
    \item[] Visualization - the least amount of time spent on this task gives 1 point.
    \item[] Limitations - if there are any limitations, the language gets 0 points.
    \item[] Workarounds - additional time to solve the task gives 0 points (deviation from the time for the naive implementation).
\end{enumerate}
The results of the programs will be also compared. If they differ, the models will be cross-compared wit the other program or manually.\\
The task is to build a best matching linear regression (if possible) for a company (always the first column in the file) using certain limitations on the number of the predictors.

\subsection{Data preparation}
As was mentioned before the data set for this task will be prepared in advance. For our purpose we decided to take Intel as a dependent variable in the regression. Predictors will be chosen among the companies from the same market of micro controllers, supplier market and customers market.\\
The Intel's competitors can be found on Wikipedia listed in several tables for years from 1998 to 2013. We used a parser to get following list of manufacturers for the years 2000-2013 for semiconductors market:
\begin{verbatim}
['AMD', 'Qualcomm', 'Micron Technology','Hynix',
'Infineon Technologies', 'Intel Corporation', 
'STMicroelectronics', 'Texas Instruments']
\end{verbatim}
The semiconductors are the basic component for different devices, so the potential consumer-markets may vary. In this Bachelor we will concentrate on microchips (CPUs) consumers, following markets are: Tablets, Smart-phones, personal computers, automobiles, video game consoles, medical technologies, Engineering technologies, Aviation. Tablets and personal computers are united into one market-group.\\
Automobile market:
\begin{verbatim}
['Toyota', 'GM', 'Volkswagen', 'Ford', 'Nissan', 
'Fiat Chrysler Automobiles', 'Honda', 'PSA', 'BMW',
'Daimler AG', 'Mitsubishi', 'Tata', 'Fuji']
\end{verbatim}
The only significant players (manufacturers) on the game console market are: Microsoft, Sony and Nintendo.\\ 
The aviation market is presented by following companies: 
\begin{verbatim}
['Boeing', 'United Technologies', 'Lockheed Martin',
'Honeywell International', 'General Dynamics',
'BAE Systems', 'Northrop Grumman', 'Raytheon',
'Rolls Royce', 'Textron', 'Embraer', 'Spirit AeroSystems Holdings Inc.']
\end{verbatim}
The next market consuming microchips and other Intel-production is Smart-phones, Tablets and PCs. These gadgets are not substitutes, but they are all very similar in production-process, end-consumer and functions. Since many companies are presented on the markets mentioned above and have further production markets, we will put them into "Diversified" category. The diversified companies that may influence Intel are:
\begin{verbatim}
['Samsung', 'Apple', 'Microsoft', 'Nokia', 'Sony', 'LG',
'Motorola',  'Lenovo',  'BlackBerry', 'Alcatel', 'Vodafone']
\end{verbatim} 
Another huge consumer market for Intel is medical equipment market. The following companies present this sphere:
\begin{verbatim}
['Johnson & Johnson', 'General Electric Co.', 'Medtronic Inc.',
'Siemens AG', 'Baxter International Inc.', 
'Fresenius Medical Care AG & Co.', 'Koninklijke Philips',
'Cardinal Health Inc.', 'Novartis AG', 'Stryker Corp.',
'Becton, Dickinson and Co.', 'Boston Scientific Corp.',
'Allergan Inc.', 'St. Jude Medical Inc.', '3M Co.',
'Abbott Laboratories', 'Zimmer Holdings Inc.', 
'Smith & Nephew plc', 'Olympus Corp.', 'Bayer AG',
'CR Bard Inc.', 'Varian Medical Systems Inc.',
'DENTSPLY International Inc.', 'Hologic Inc.', 
'Danaher Corp.', 'Edwards Lifesciences', 'Intuitive Surgical Inc.']
\end{verbatim}
Additional to above mentioned companies several big players from the industrial equipment market will be added. These are following companies:
\begin{verbatim}
['ABB Robotics', 'Adept Technology', 'Bosch', 'Caterpillar',
'Denso Robotics', 'Google', 'Universal Electronics']
\end{verbatim}
The list of the potential predictors is not full, because some pretty big players on the markets are left due to the lack of information. The reason of the information-lack is recent enter to the international stock exchange (since 2010 earliest). Some companies are still closed for the foreign investors (which is the case for such giants as Samsung, Honda and other Asian companies). The last limit on chosen companies is the trading-volume. We simply can not use the company's data for composing a regression, if the last year or two the trade volumes for the shares was 0.\\
After the companies were chosen, the whole data table will be split into 2 data sets: learning set and validation set. We have obtained daily prices from 2006 to 2015 (31.12.2014 is the last date for all indexes). The training or learning set will be approximately 70\% from the whole data volume: from 2006 to 2010. The validation-set is about 30\%: from 2011 to 2012. For this work we use daily frequency (only opening prices).\\

\newpage
\section{Python}
\subsection{The structure of the program}
The program consists of three main classes, connected with each other in main class. The first class is called DataFormating.py and is responsible for getting data out of the csv file and writing it into an instance of a proper format for other two classes, also the dependent variable will be extracted from the whole data base (the dependent variable is always the first column in the csv table).\\
The second class is called StatisticTests.py, it runs several test to find out main static characteristics in order to choose an appropriate model class for the data.\\
The third class is called BuildModel.py. This class builds the multiple linear regression (according to the results of the StatisticTests class) using step-forward approach:
\begin{enumerate}
    \item The amount of companies taken into account is reduced using correlation: all companies with correlation less than 30\% are left out.
    \item A limit on the maximum number of the parameters is set using following rule: 1 company out of 10 if number of company exceeds 5.
    \item The first step is to find the best one parameter company using the highest correlation coefficient from the first step.
    \item best models among the classes are compared using likelihood ratio test. 
    \item If the bigger model is better and the limit of the predictors in regression is not achieved, the previous two steps are repeated.
\end{enumerate} 
At the end the full information for the best model is returned.

\subsection{The process description}
In this section the process of the coding is described: what difficulties and problems are encountered and how they were solved.
\subsubsection{Used Libraries}
In the course of writing the program several steps were implemented. First one, as was told above, is the data extracting. Although, python language has a native library to read csv files and format them into a list or a dictionary, in this work pandas library is used [15]. \\
The class responsible for statistic test uses three different libraries and sources: Scipy [16], Statsmodels [17] and Scikit [18].\\
Scipy library was mainly used to perform goodness of fit Kolmogorov distribution tests and to build KDE and CDF. The other two libraries could do the same, but in this case the use of the class was better explained for Scipy.\\
The Statsmodels library is used separately for StatisticTests class and BuildModel class. In the first part, Statmodels is used to run Augmented Dickeyâ€“Fuller test [5] to figure out whether data is stationary.\\
The Sklearn module from Scikit was used mainly during testing of the functionality of the program. The library can perform the same tests that were mentioned above, but with unnecessary complication of the documentary and methods use.\\
The Matplotlib library is used to present the KDE and CDF graphically. Normally this library is used as the part of the Scipy source, so it is used in this work.\\ 
In the BuildModel class two libraries were used directly: Statsmodels and Numpy. Tne Numpy library is a part of the Scipy package, but in this case it was used explicitly. The main and the only function of the Numpy was to speed up mathematical computations. Of course, we could have taken build-in Math class directly, but it is a common and a good practice to use Numpy for mid-difficult computations, like:
\begin{verbatim}
element = numpy.corrcoef(self.dict_data[self.list_companies[0]],
                         self.dict_data[key])[0][1]
\end{verbatim}
This element computes the correlation between the dependent variable and any other variable from the data. The Math class can present simple operations, in order to compute the coefficient of the correlation we should have built the formula directly without a wrapper function. And every part of the computation would have taken longer. \textcolor{red}{Do I need to present a benchmark results here?}\\
The Statmodels library is used directly to build a linear regression using Generalized Least Squares [19] method. GLS was chosen to eliminate some possible insignificance of Ordinary Least Squares or Weighted Least Squares.\\
The Likelihood ratio test [12] function was implemented in Statmodels, however we found it unusable, since the resulting output had completely different type, as we have expected. Looking into the source code of the wrapper function didn't bring much, this is why the LLR-test was implemented directly according to the formula and was yielding results of the proper format.\\
The end result is currently printed out to a console: The names of the companies in the final regression, main characteristics of the regression (determination coefficient, AIC, BIC, Loglikelihood, coefficients and errors, etc.) and final run time of the program. It is possible to show the full information for each iteration the algorithm makes, but it will take more time to compute. 

\subsubsection{Documentation quality and quantity}
Since all libraries a very big and some of them a pretty complex, the quality of the documentation played a big role in choosing the libraries and packages for the program. Originally, Python language was used a lot by the scientific society and many libraries were also implemented for scientific use for people with certain background. This is why there were some difficulties with several libraries. The first issue appeared while choosing the proper library for computing the linear regression. The second issue appeared during writing tests for the data.\\
The Scikit library offered many advanced or very specific models, the also have implemented some not very popular methods for beginners. However the documentation is pretty good, all input parameters are explained up to format. The output results also are explained in details. The links to the source code are given to every method of the class and not only to the page. There are different number of examples for each function and each class. The bad thing about Scikit documentation is that they don't always provide the link to the theoretical background: this makes it harder to understand how and what exactly is computed in the method or function. On the page the also provide users with the links to other useful resources (all of the resources used in this work are mentioned on the site). A very big advantage of the library is hosting all the documentation on their own server. The code is always available on Github and the project is opened for everyone. \textcolor{red}{figure out where they are actually hosting}\\
The Scipy source, as was already mentioned before, unites 6 libraries and thus covers the whole process of getting the data, analyzing the data and giving back the results using visualization tools. The documentation can come in two forms:documentation for the wrapper for specific function or class of one of the components, or the direct documentation of the library. All the libraries and tools from Scipy have very detailed documentation that they host on their servers. Most of the documentation is presented in typical style python uses. Which is an advantage, if you are used to it. There are many tutorials also offered on every site. Sometimes they even compare the library with R language [20]. Scipy and every all its libraries hold their code on Github, although the links to source code are not always shown in the documentation. In this work the tools united in Scipy were sometimes used explicitly, but the general documentation overview of the Scipy library fits all of its elements documentation.\\
The Statmodels library proved to be easy in use, if you understand how it is all organized. This library has a good content tree, but contains not enough general and small specific examples. The problem is sometimes to understand what input parameters are needed and why the result has unexpected format. For instance, in the documentation of some functions parameters, methods and return instances are organized not by appearance or relevance order, but by alphabet, which makes it many times harder to read it. The source code is given for the whole class and not for the certain parts. However, they give at least one link to the theory behind the implementation. One of the biggest inconveniences was being unable to read the documentation. The Statmodels keep their documentation at sourceforge. This resource had been offline for several days in a row, which can be a very big issue for a project with a deadline. The code is kept on Github. It is very hard to read the code without certain theoretical and practical background. Although Statmodels have many disadvantages, this library was chosen for the program because of its content. In most cases the resulting output had proper (suitable for our program) format and did the exact thing we wanted.\\

\subsubsection{Advantages and disadvantages of the program}
The results of the program for the given data, described in the data preparation section are:
\begin{verbatim}
The best model contains  6  parameters. And the model is:
['STMElectro', 'Olympus', 'St Jude', 'Lenovo', 'MicronTech', 'Google']
                            GLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.998
Model:                            GLS   Adj. R-squared:                  0.998
Method:                 Least Squares   F-statistic:                 8.701e+04
Date:                 Do, 24 Sep 2015   Prob (F-statistic):               0.00
Time:                        20:20:45   Log-Likelihood:                -1753.3
No. Observations:                1239   AIC:                             3519.
Df Residuals:                    1233   BIC:                             3549.
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1             0.1254      0.016      7.933      0.000         0.094     0.156
x2             0.0609      0.012      5.065      0.000         0.037     0.085
x3             0.3261      0.016     21.029      0.000         0.296     0.357
x4             0.2126      0.004     47.272      0.000         0.204     0.221
x5             0.0083      0.001     12.929      0.000         0.007     0.010
x6             0.1291      0.012     10.536      0.000         0.105     0.153
==============================================================================
Omnibus:                       10.831   Durbin-Watson:                   0.120
Prob(Omnibus):                  0.004   Jarque-Bera (JB):               10.859
Skew:                           0.212   Prob(JB):                      0.00439
Kurtosis:                       2.827   Cond. No.                         357.
==============================================================================

[Finished in 13590.5s]
\end{verbatim}
The overall run time is pretty big - 3,7 Hours. The main time consumption comes from StatisticTests.py, specifically from stationarity test.
\textcolor{red}{verbatim - run time}
This section was written pretty naive and thus is slow. The advantage of the python is its multipurpose nature. On the functions changing big arrays or different data type it is possible to use speed up methods like Multiprocessing [21]. This will give you up to 8 times speed up momentum (depending on number of cores you can use). However this a bit of intermediate knowledge of python programming and will be quite difficult for a beginner to use.
\subsection{Results}

\newpage
\section{R}
\subsection{The structure of the program}
The program written in R language has almost exact the same structure as the Python program, as far as the language allows. 
\subsection{The process description}
\subsubsection{Used Libraries}
\subsubsection{Documentation quality and quantity}
\subsubsection{Advantages and disadvantages of the program}
\subsection{Results}

\newpage
\section{Results}
\subsection{Objective comparison}
\subsection{Subjective comparison}
\subsection{Conclusion}

\newpage
\section{Literature}
\begin{enumerate}
    \item[1] \url{https://en.wikipedia.org/wiki/Python_(programming_language)}
    \item[2]
    \item[3] \url{https://www.r-project.org/about.html}
    \item[4] \url{http://www.revolutionanalytics.com/what-r}
    \item[5] \url{https://en.wikipedia.org/wiki/R_(programming_language)}
    \item[6] \url{http://stackoverflow.com/questions/2770030/r-or-python-for-file-manipulation}
    \item[7] \url{http://datascience.stackexchange.com/questions/326/python-vs-r-for-machine-learning}
    \item[8] \url{http://www.kdnuggets.com/2015/05/r-vs-python-data-science.html}
    \item[9] \url{http://blog.datacamp.com/r-or-python-for-data-analysis/}
    \item[10] \url{http://101.datascience.community/2015/05/12/data-science-wars-r-vs-python/}
    \item[11]
\end{enumerate}
\end{document}