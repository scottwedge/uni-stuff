\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{color}
\usepackage{mathtools}


\title{Bachelor 2015}
\author{Alisa Dammer}

\begin{document}
\maketitle
 
\begin{enumerate}
	\item[1.] Introduction:
		\begin{enumerate}
			\item[1.1] Motivation
			\item[1.2] Overview of the existing approaches, models and programs
			\item[1.3] Sentiment data analysis
		\end{enumerate}
	\item[2.] The model
		\begin{enumerate}
			\item[2.0] Explanation of the Thesis of the Bachelor
				\begin{enumerate}
					\item[2.0.1] Theoretical background and the problem statement
					\item[2.0.2] The program broken into several steps
						\begin{enumerate}
							\item[2.0.2.1] Data preparation
							\item[2.0.2.2] Model fine-tuning
							\item[2.0.2.3] Checking the hypothesis for specified limitations
						\end{enumerate}
				\end{enumerate} 
			\item[2.1] Data preparation (correlation matrices and further filtering)	
				\begin{enumerate}
					\item[2.1.1] Choosing dependent markets and companies listed on stock exchange
					\item[2.1.2] Splitting the data, Correlation matrix and first cut-off
					\item[2.1.3] Preparing the data for further usage
				\end{enumerate}
			\item[2.2] First algorithm
				\begin{enumerate}
					\item[2.2.1] Estimating the parameters
					\item[2.2.2] Getting the names of the explanatory variables
				\end{enumerate}
			\item[2.3] Second Algorithm
				\begin{enumerate}
					\item[2.3.1] Data collection
					\item[2.3.2] Data analysing
					\item[2.3.3] Interpretation of the results. Create an input for the first algorithm
				\end{enumerate}
			\item[2.4] Re-estimating the parameters of the model 
			\item[2.5] Result testing
		\end{enumerate}
	\item[3.] Conclusion
		\begin{enumerate}
			\item[3.1] The interpretation of the results
			\item[3.2] Bottle neck and other problems of the model
			\item[3.3] The potential and further use
			\item[3.4] Possible improvements
		\end{enumerate}
	\item[4.] References
\end{enumerate}

 ewpage
\section{Introduction}
\subsection{Motivation}
Big Data\\
Data mining\\
Sentiment data analysis
\subsection{An overview of the existing approaches, models and programs}
classical econometric - regressions\\
Among the classical econometric approaches two main directions can be called: linear and non-linear regressions. First of all we will consider linear regressions and discuss their advantages and disadvantages.\\
According to the most common definition, in statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable y and explanatory variable denoted X [1]. There are one-variable regressions called simple linear regressions and multiple regressions, which shows the dependencies of the dependent variable from more than one explanatory variables. The regression equation looks following way:\\
\[Y = a + bX + \varepsilon\]
where Y - dependent variable, X - explanatory variable, a - the intercept of regression line, b - the slope of the regression line, $\varepsilon$ - the error term.\\
The simple linear regression is based on 6 strict assumptions:
\begin{enumerate}
	\item Because of the linear form neither parameter a, nor parameter b may have higher power than one.
	\item The independent variable X is not random.
	\item The variance is constant. Which means that the expected error value is 0.
	\item The variance of $\varepsilon$ is constant for all observations.
	\item Errors are not correlated.
	\item The error's distribution is normal.
\end{enumerate}
On one hand the simple linear regression has several disadvantages: the form of the equation (here number of the predicting variables is concerned) and underlying assumptions. The dependency from only one variable can not describe fully all the relationship on the market. Even if you consider an equations system, consisting from linear regressions with different variables, it will lead to unnecessary complexity and not realistic or complete dependencies between variables. The second disadvantage is the assumptions - they are unrealistic in modern world and are appropriate only for very simplified model of he world.\\
The big advantage of the simple linear regression on the other hand is it's estimators: ordinary lest squares, generalized least squares, instrumental variables, maximum likelihood (ML) and various ML techniques - these are the most known and not too complex approaches to estimate the regression. There also many other different techniques, but we won't discuss them here. An example of a simple linear regression could be dependency between the harvest volume and amount of rain during the season. Let's say Y = amount of harvest (potato for example) in kg and X is amount of the precipitation in mm\\ Than the more rain will drop during the season the bigger amount of potato will be harvested at the end. The slope of the regression line is positive. And the intercept is also more than 0, because you can't dig out less, than you have planted. So, the regression will look like:
\[amount \thinspace of \thinspace harvest = |a| + |b|*amount \thinspace of \thinspace rain \epsilon\] \\
Since it is likely to be impossible to build a proper prediction for the variable only with one predicting variable, multiple linear regression was invented. As you can see from the name, multiple linear regression describes relationship between more than one explanatory variables and dependent variable. The form of the equation looks as following:\[ Y = a + b_{1}X_{1}+ ... + b_{n}X_{n} + \epsilon, \thinspace \thinspace where \thinspace \thinspace n \in N\]   
$b_{i}$ - the "contribution" of the i-th variable to the regression, a -the intercept of the regression line, Y - dependant variable, X - explanatory variables.\\
Unlike simple linear regression, multiple linear regression does not have the form-disadvantage. However, the linear nature of the regression provides the model with several assumptions:
\begin{enumerate}
	\item All $b_{i}$ have the power of 1 - linear dependency between Y and all Xi-th.
	\item The residuals are distributed normally. (Residuals - the difference between predicted value and observed value).
	\item Uncertain number of the explanatory variables X - there is no technique to choose exact number of the variables that will optimally predict the value Y.
	\item Completely "substitutive" variables X - unnecessary big number of the variables without increasing the accuracy of the prediction.
	\item Complex form of the variables (X can have a higher-order polynomial form) leads to reduction of transparency and wrong results. 
\end{enumerate}
The advantage of the multiple linear regression is that it can be tested several ways. First, you can test whether the regression has the linear character by using F-test or ANOVA table. Second, to check whether the model is good enough you can use the same tests as for simple linear regression: determination coefficient $R^2$. You can also run several hypothesis-test for each explanatory variable ($b_{i}$ coefficient is equal to 0 - the variable is insignificant).\\
As an example of the multiple linear regression we can take previous example and extend it. The amount of the precipitation is not the only factor for the harvest to grow. Our Y variable also depends on amount of sunny days ($X_{2}$) and average temperature during the season ($X_{3}$). The higher temperature, the better will be the harvest (in reality this is a non linear dependency; most likely the dependency here takes the form of $(c-d*\sqrt{x})$: the harvest will be big, if the temperature is between certain degrees). The higher number of the sunny days the less rainy days, but higher the temperature. This is an example where two explanatory variables are not complete substitutes, but they have negative dependency with each other, so, their dependency will be explicitly included into the regression. The general form of the equation will take following form:
\[harvest = |a| + |b_{1}|*rain + |b_{2}|*sun + |b_{3}|*temperature + |b_{12}|*sun*rain + \epsilon\]
So the multiple linear regression is better for real purposes (real market analysis) in comparison to simple linear regression, but it still contains restrictions that do not allow high accuracy prediction.\\
Unfortunately linear dependencies between all the parameters and variables in the equation is a rare thing in the real life. That is why nonlinear regressions took place in statistical analysis. The general form of the nonlinear regression looks as following:
\[Y = f(\beta, x'_{i}) + \epsilon'\]
where f() - nonlinear function, $x'_{i}$ - vector of predictors, $\epsilon'$ - vector of parameters, $\epsilon_{i} \sim N(0, \sigma^2)$.\\
Nonlinear regressions can be divided into two main classes:
\begin{enumerate}
	\item Nonlinear dependencies between explanatory variables ($x_{i}$) and dependent variable (Y), linear character of the parameters ($\beta$).
	\item Linear dependencies between explanatory variables ($x_{i}$) and dependent variable (Y), nonlinear character or the parameters ($\beta$) 
\end{enumerate} 
The first class is relatively easy to work with: you can use variables substitution and transform the original equation to the linear form; than use the same techniques as for the linear multiple regression and find estimates for the parameters. For example, you can use least squares method with additional steps. To use LS method you will first need to change the form of the variables to the linear form: $z_{i}$ = $f^{-1}(x_{i})$, where z is a new variable in a linear form. The equation will then take following form: $y=a+b_{1}z_{1}+...b_{n}z_{n}$. As the next step your find the estimates using LS method. However, the estimates will be biased towards original dependencies, because we found them for variables $z_{i}$, which are the transformation of the original variables [non linear j]. So you need to do the last transformation of the estimates for original variables $x_{i}$.\\
The second class is more difficult to use. Because of the nonlinear nature of the parameters you can not use normal estimation techniques. On one hand you can use the ordinary optimum finding algorithms, but you will run into different difficulties because of the character of the dependencies. There are exist different approaches to get the estimates for the regressions: Gauss-Newton method [nonlinear m],  Levenberg-Marquardt method [nonlinear n] (we will not concentrate on these approaches in our work, for further information you can read sources from reference-list). \textcolor{blue}{You can also iteratively try to use LS method (iterative transformations), but with each iteration the estimates will get additional error, which can result in immense accuracy decrease}. \\
As an example for a nonlinear regression we can take the earlier described dependency between harvest volume, amount of rain, number of sunny days and temperature.
\[harvest = a + b_{1}*ln(sun) + b_{2}*(c-d*(rain-e)^2) +b_{3}*f(temperature) + b_{12}*g(sun, rain)\]
where $f(n) = \left\{ 
  \begin{array}{l l}
  	0,\thinspace x < 10 \\
    \alpha x, \thinspace x \in [10, 30] and \alpha > 1 \\
    -\alpha x,\thinspace x > 30
  \end{array} \right.$ ; c, d and e $>$ 0, $b_{i}$ - reggresion coefficients here they are first-order parameters. \\
As you can see,the normal approach here is is almost impossible to use, moreover, here we see different form of dependencies at one time (normally people use uniform functions like polynomials, logarithms etc.) But this equation can be transformed into linear form via several steps of the variables substitution and transformations, then the LS will be found and the process of "backwards transformation" will be started, until we get the estimates for the original variables.

neural networks\\
The biggest disadvantage of the classical regression models on one hand  is increasing complexity when using too many variables and parameters. Also, they are not flexible enough for a quickly changing and constantly growing and changing financial markets. On the other hand, when using too few parameters and variables the model looses a lot of information and flexibility. To overcome these problems neural networks are used to make predictions and analyze the increasing amount of data. According to Wikipedia:"In machine learning, artificial neural networks (ANNs) are a family of statistical learning algorithms inspired by biological neural networks (the central nervous systems of animals, in particular the brain) and are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are generally presented as systems of interconnected "neurons" which can compute values from inputs, and are capable of machine learning as well as pattern recognition thanks to their adaptive nature" [ANN a]. The ANNs ae used in different spheres where pattern recognition can be used. Only the use of NNs on financial markets is relevant for this work, so we will only consider appropriate forms of the networks and learning-mechanisms.\\
The basic structure of the ANN consists of 3 layers: input, hidden and output layer. As follows from the name, the bottom layer is a input layer, that sends all the data (in our case prices, time series) to the next level via "synopses". Synopses in ANNs are the dependencies between input units and the units of the hidden layer (a weight-function). For example,
\[y_{j} = \sum_{i \in I}w_{ij}x_{i} \]
where $y_{j}$ - an unit in the hidden layer, $w_{ij}$ -weight for input unit i in jth hidden unit (influence if the input node on the hidden node), and $x_{i}$ - input unit.\\
Next dependeny-level is the transformation the hidden layers into output. (This dependency might look the same as the previous one). As an example of the model following form can be used:
\[z=f(y)\]
where z - an output unit, $f$ - a function that transforms all hidden units $y_{j}$ into the output ($y_{j}$ is equals to the sum of the weighted input units, see above), here y is a vector of $y_{j}$, $y=(y_{1},...,y_{n})$.\\
Not only predefined functions are important in building an ANN, but also the way the network will be trained. The advantage of the ANNs is the possibility to adapt (search for some sub-optimal transformation function $f^*(y)$). In literature you will find the definition of "cost function". Minimum of the cost function (normally this function depends on the observations or input data,for example, the coefficient of the determination can be used as a cost function) will signal, that our Network has reached some local optimum. (it is hard to find the global optimum and moreover to prove, that found one is indeed the global optimum). The learning process can be different, depending on the goal. However, the main steps are the same:\\
\begin{enumerate}
	\item The whole data set is divided into several chunks: training set, testing set and validation set. 
	\item The training data set is properly processed before the training (learning) phase starts. The data needs to be cleared out of noise and additional shifts for the network to get the pattern properly.
	\item After the training on the train set, the cost function will be compared with a desirable state. The parameters will be fine-tuned if need.
	\item The network with fine-tuned parameters will be tested on the test-set. If needed this and previous steps will be done again.
	\item The "trained" network will be validated on the validation test.
\end{enumerate}
ANNs have several advantages and disadvantages, that make them not that easy to use. The main advantages are the flexibility and ability to work with immense amount of the data. By flexibility here we mean, that the network is able to recognize the pattern and its development, if there any change over time. By immense amount of data here we mean, that the model can contain incredible amount of input nodes, as well as several hidden layers, each containing huge number of nodes. Such structure allows to recognize different patterns at once and model the behavior of the dependent variables very accurate.\\
On the other hand, the disadvantages are the run time, no opportunity to debug the network on the fly and initial parameters. Let's start with the initial parameters problem. The network should be given some assumptions about the data set, its behavior and possible dependencies. These assumptions result in an initial weight matrices or parameters for the initial dependency functions. If the parameters were poorly estimated and contain serious errors, the network might even show the opposite behavior to optimum finding. The structure of the NN must be defined before the training. Depending on the initial assumptions and the goal of the training, the structure may vary from Deep Boltzmann Machine (only visible and hidden units: one hidden layer - containing up to 10 nodes in total) to incredibly huge perceptron (5 hidden layers, each containing more than starting from 300 nodes). The bigger and complexer is the structure, the more parameters should be estimated in the beginning, the more time will the NN need to be trained. Another aspect of the complexity problem is the way the NN is going to be trained. Most famous approaches are bottom-up and top-down techniques. The learning method can be incorrectly used or wrongly assigned to the network, which will result in low accuracy prediction and pattern recognition. The second problem - the run time depends on the structure of the ANN, amount of the parameters and the size of the training data set. It can only increase with adding new parameters to the model. The third problem is that you won't know whether the model is good until it is validated. There is no possibility to stop the training process and debug the model. The process of the training is automatic for the network. If you will interrupt current training session and fine-tune the parameters manually, that will mean the new training session without any progress saved for the model.\\
However the the complexity of the ANNs is compensated through their ability to recognize the patters in changing world, their ability to adapt to changing market structure and new data. \textcolor{red}{Unfortunately this is a complete overkill for this bachelor, and I am not really sure how to use it for my hypothesis anyhow, mostly because of the no possibility to debug it. Most time will be spend on waiting for the network to finish the training and testing stages. It can be later used as a combination of my hypothesis and "advanced forecasting in the next work} \\
Generally speaking, all "self-developing" models can be referred to machine learning. According to Wikipedia:"Machine learning is a scientific discipline that explores the construction and study of algorithms that can learn from data. Such algorithms operate by building a model based on inputs and using that to make predictions or decisions, rather than following only explicitly programmed instructions" [machine learning a]. Machine learning defines several steps for ANNs for instance. The way you can teach your AN is first defined in machine learning field. Also the rules for different data sets are defined in this scientific sphere. Building a regression can also be viewed as a machine learning problem, especially if the regression is built for changing environment. Machine learning can be used as for short- and midterm predictions and for strategical decision support (trend prediction and long-term prediction have shown better results in some papers). For this bachelor learning rule (supervised or unsupervised), data sets partitioning are important.  \\
Not only models develop to increase the accuracy of prediction, but also tools and software for data collecting and analyzing. The quality and amount of data increases exponentially and old approaches becoming unusable.\\
relatively new trend in data analysis is the sentimental data analysis. The big role for stock exchange plays the psychological component: the way investors predict and react on certain events, the way they collect and share information. Social media has become a source of important information for decision making in all short-, mid- and long-term. According to Wikipedia:"Sentiment analysis (also known as opinion mining) refers to the use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials" [sentiment a]. The sentimental analysis combines different approaches and models to detect the mood, key words (proper information) and the insensitivity of the discussion (if more than one source is analyzed) or the how the object is important for the person. The sentiment data can not only give a prediction for financial tools on the markets, but also give a hint about the components of the whole behavior. We know, that the price of the share, for example, reflects the real value of the firm, the business reputation (what people expect of the firm), noise and external extreme factors (not ordinary shifts, gaps and jumps). The sentimental data analysis combined with classical fundamental and technical analysis can result in high accuracy prediction for all time periods: also for portfolio strategies and simple trading.

\newpage
\section{The model}
\subsection{Explanation of the Thesis of the Bachelor}
The Idea of the work is to combine classical technical analysis and sentimental analysis and use them indirectly. The first part is to choose the most appropriate regression for the dependent variable. The result of the first step is the names of the explanatory variables, we will need them for further work. The second step is to get predictions for each explanatory variable using sentimental data analysis. For each of the variables in the found optimum we will search for text data and analyze it for predictions, buzz (how frequent will be discussed the topic) and mood. The third step is to use predicted values of the explanatory variables in the original regression model to get prediction for the dependent variable. At the same time mood from the sentimental data and buzz will fine-tune the parameters in the original equation. As the result we will get new time series for testing (predictions from text analysis) and fine-tuned parameters. In simple words, the sentimental data analysis is used for fine-tuning the parameters and including psychological component to the strict regression model. To validate the final equation test set will be used: the predicted values of the dependent variables will be compared with the actual value. If the difference will stay within 75\% of accuracy, the approach for fine-tuning the parameters of the equation can be considered efficient.\\
The whole methodology can be broken into two main algorithms, each algorithm can be broken into several steps and additional forward and backward connection-steps between the algorithms:
\begin{enumerate}
	\item The data in form of the time-series for most appropriate indexes will be collected. The data will be divided into three chunks: training set, test set and validation set. 
	\item For the chosen equation and chosen dependent variable (regression model) iterative the best combination of the explanatory variables will be chosen. The parameters estimated based on the training set.
	\item For found dependent variables text sources containing relevant information will be collected and analyzed. As the result we get the values of the explanatory variables in time-series form, buzz in numeric form and mood in form of $(-1)^n$, where n=0, if the mood is positive and n=1, if the mood is negative.
	\item Based on the new time series the parameters of the model will be re-estimated and fine-tuned by mood and buzz (buzz in this case is time-dependent weight modifier as well as the mood).
	\item New form of the regression will be again tested on the test set, and if needed, the parameters will be fine-tuned again. 
	\item After testing, we will get the forecast for certain period for the dependent variable and compare it with the actual value from the validation set. 
	\item Based on the difference between actual and predicted value the approach can be considered good or bad.  
\end{enumerate}
In simple words, the goal of this bachelor is to test the ability of the sentimental data analysis to make classical regression models more flexible and time-responsive.

\subsection{Data preparation}
\subsubsection{Dependent markets}
In this work we decided to test our hypothesis on Intel. Intel is a multinational corporation, with a long history behind.\\
The reason why we have chosen Intel is that is quite transparent company. Transparency is defined here as following characteristics:
\begin{enumerate}
	\item Intel is included into most global composite indexes.
	\item Intel is not a diversified company. It's main market is semiconductor chips (we can even extend the market to basic electronics for computers). 
	\item The Intel's market has pretty clear borders and is not very effected by external factors (government policy). 
	\item The company is not influenced directly by other huge players of the electronic market (no blocking amount of shares, partnership projects only).
\end{enumerate}
Intel suits our analysis, because it is a stable company with data over many years. The correlated indexes are also pretty clear and the amount of explanatory variables can be limited without any drop in accuracy.\\
As the explanatory variables we will choose following categories:
\begin{enumerate}
	\item competitors
	\item suppliers
	\item customers
\end{enumerate} 
The competitors can be found on Wikipedia listed in several tables for years from 1998 to 2013. We used a parser to get following list of manufacturers for the years 2000-2013 for semiconductors market:
\begin{verbatim}
['AMD', 'Qualcomm', 'Micron Technology','Hynix',
'Infineon Technologies', 'Intel Corporation', 
'STMicroelectronics', 'Texas Instruments']
\end{verbatim}
For these ten companies the daily data from 2000 to 1.01.2015 (31.12.2014) will is collected.\\
Customers of the market influence the companies-manufacturers directly. The probability, that at least one customer-company will be used in the final regression, is high. The data for biggest customers-market and the players on the market will be collected the same way, as it was done with the manufacturer-companies.
The semiconductors are the base for different devices, so the potential consumer-markets also differ a lot. In this Bachelor we will concentrate on microchips (CPUs) consumers. Another limitation is the character of the market itself. Nowadays we are surrounded by gadgets with mat least one CPU inside. So, the potential consumers are different on scale, complexity, volume etc.. Here we consider the companies from opened markets (no high entrance barriers, no government orders, no military use, transparent documentation and opened to every type of the investors). Following markets are: Tablets, Smart-phones, personal computers, auto-mobiles, video game consoles, medical technologies, Engineering technologies, Aviation. Tablets and personal computers are united into one market-group. Although the smart-phone market is self-sufficient, the players on the market are also have production on personal computers market.\\
Automobile market:
\begin{verbatim}
['Toyota', 'GM', 'Volkswagen', 'Ford', 'Nissan', 
'Fiat Chrysler Automobiles', 'Honda', 'PSA', 'BMW',
'Daimler AG', 'Mitsubishi', 'Tata', 'Fuji']
\end{verbatim}

The only significant players (manufacturers) on the game console market are: Microsoft, Sony and Nintendo.\\ 
The aviation market is presented by following companies: 
\begin{verbatim}
['Boeing', 'United Technologies', 'Lockheed Martin',
'Honeywell International', 'General Dynamics',
'BAE Systems', 'Northrop Grumman', 'Raytheon',
'Rolls Royce', 'Textron', 'Embraer', 'Spirit AeroSystems Holdings Inc.']
\end{verbatim}
The next market consuming microchips and other Intel-production is Smart-phones, Tablets and PCs. These gadgets are not substitutes, but they are all very similar in production-process, end-consumer and functions. Most of the following companies are presented on mentioned markets and have further production markets. That is why we will put them into "Diversified" category. The diversified companies that can influence Intel are:
\begin{verbatim}
['Samsung', 'Apple', 'Microsoft', 'Nokia', 'Sony', 'LG',
'Motorola',  'Lenovo',  'BlackBerry', 'Alcatel', 'Vodafone']
\end{verbatim} 
Another huge consumer market for Intel is medical equipment market. The following companies present this sphere:
\begin{verbatim}
['Johnson & Johnson', 'General Electric Co.', 'Medtronic Inc.',
'Siemens AG', 'Baxter International Inc.', 
'Fresenius Medical Care AG & Co.', 'Koninklijke Philips',
'Cardinal Health Inc.', 'Novartis AG', 'Stryker Corp.',
'Becton, Dickinson and Co.', 'Boston Scientific Corp.',
'Allergan Inc.', 'St. Jude Medical Inc.', '3M Co.',
'Abbott Laboratories', 'Zimmer Holdings Inc.', 
'Smith & Nephew plc', 'Olympus Corp.', 'Bayer AG',
'CR Bard Inc.', 'Varian Medical Systems Inc.',
'DENTSPLY International Inc.', 'Hologic Inc.', 
'Danaher Corp.', 'Edwards Lifesciences', 'Intuitive Surgical Inc.']
\end{verbatim}
\textcolor{red}{Baxter is missing}\\
Additional to above mentioned companies several big players from the industrial equipment market will be added. These are following companies:
\begin{verbatim}
['ABB Robotics', 'Adept Technology', 'Bosch', 'Caterpillar',
'Denso Robotics', 'Google', 'Universal Electronics']
\end{verbatim}
Intel diversified their structure that way, so that they have reduced the transportation and production costs, and made their production more flexible. This why we don't have to consider the suppliers companies in his work.\\
Of course, these are not all companies that should be considered to establish a good model. 
Some pretty big players on the connected markets are left out because of the lack of information. The reason of the information-lack is that some of the companies have only recently entered the international stock exchange (since 2010 earliest). Other companies are still closed to the foreign investors (which is the case for such giants as Samsung, Honda and other Asian companies). The last case is the trading-volume limitations (sometimes restricted to 0 for more than a week).\\
But our first goal is to establish a more or less working model (bad model is actually preferable. This will be explained in details in the results interpretation). We want our original model to be really bad and not flexible, that is why we have limited the number of variables to iterate through.
\subsubsection{Splitting the data}
Next step, after obtaining the prices on the stock for all mentioned above companies, is to split the whole data-set into three chunks: learning set, testing set and validation set. We have obtained daily prices from 2006 to 2015 (31.12.2014 is the last date for all indexes). The training or learning set will be approximately 60\% of the whole data volume: from 2006 to 2010. The test-set is about 20\%: from 2011 to 2012. And the last validation set is the rest 20\%: from 2013 to 2014. For this work we use daily frequency (only opening prices).\\
After obtaining all the data and splitting into three data chunks we start first data analysis to create a limited data. This data set will contain the information (company name and time series) for companies, which are relevant for further analysis. The first-order cut-off is made based on correlations with Intel-company: all companies with correlation coefficient smaller than 30\% are left out. \\
\textcolor{blue}{correlation matrix}\\
According to the results of the first-order cut-off 10 companies are left out.
These are:
\begin{verbatim}
['Bard', 'Qualcomm', 'Apple', 'Abbott', 'Bosch', 'Adept', 'AMD', 
'Edwards', 'Allergan', 'VW']
\end{verbatim}
For the remaining data we will run several test to find out the statistical characteristics. These characteristics are needed to built proper regression and use proper estimation methods for the parameters.
\subsubsection{Preprocessing the data}
The data for each company will be checked for stationary character of the process, auto correlation, distribution of the data and error's distribution and Independence.\\
According to Wikipedia:"In mathematics and statistics, a stationary process (or strict stationary process or strong stationary process) is a stochastic process whose joint probability distribution does not change when shifted in time. Consequently, parameters such as the mean and variance, if they are present, also do not change over time and do not follow any trends"[4]. Our first program will check the data for being stationary using Dickey-Fuller test [5]. \textcolor{blue}{that is a one step in the program for the first algorithm} The data which haven't passed the test is thrown out of consideration.\\
The first step in chain of the distribution-tests will be building CDF (cumulative distribution function) and KDE (kernel density distribution) for each company from the data set [8][9]. \textcolor{blue}{an example for several companies CDF, KDE}\\
From the graphs you can see that empirical distribution differs a lot from the normal distribution. However, for each company from the table we will use Kolmogorov goodness of fit test. The null-hypothesis is that the data has a normal distribution. The test has 85\% confidence interval - this is a quite generous error-assumption especially for big amount of data. However, normal distribution in financial data is quite rare. \textcolor{blue}{results of the test?}\\
The most common distributions among finance data are log normal, Levy-distribution and Pareto. Financial Data has bigger 2d and 2d moments, is normally shifted to one side and has fatter tails. So we will run tests for every above mentioned distributions with confidence level of 95\%. However, we will stop testing if one of tests returns true for null-hypothesis. We will check data belonging to a family of distribution (different parameters).\\
According to the tests results our data is log normally distributed with 95\% confidence. This means that we can actually build a linear multiple regression and use the same procedure for Maximum Likelihood estimation as for normal distribution. The explanation why we can build a linear regression for non linear distributed data is given in the first chapter. 
\\
The time series of all companies are stationary and will be checked for Auto correlation. Auto correlation according to Wikipedia:"Auto correlation, also known as serial correlation, is the cross-correlation of a signal with itself. Informally, it is the similarity between observations as a function of the time lag between them. It is a mathematical tool for finding repeating patterns, such as the presence of a periodic signal obscured by noise, or identifying the missing fundamental frequency in a signal implied by its harmonic frequencies"[6]. The data will be checked within our program with Breush-Godfrey test [7]. The same as with stationary-check: if the data is auto correlated, than it is thrown out of further consideration. 


 
\subsection{First Algorithm}
<<<<<<< HEAD
The goal of this bachelor work is to check the ability of sentimental data analysis to fine-tune forecasting models. This is why we will take a rather simple model with low predicting power - linear regression. Ob one hand, the disadvantage of this class of models is non flexibility over time, no built-in psychological component and low predicting ability, especially for complex data. On the other hand, the advantages are simple and clear structure of the model, the number of parameters we have to deal with is limited and the approaches for estimation are rather simple.\\
The main idea for the first algorithm is to choose the best possible multiple regression (here we consider polynomial regression as the sub class of the linear regression models) from a bunch of different companies.\\
In order to reduce the run time of the program several cut-off stages need to be implemented. Afterwards, the number of explanatory variables in the final equation needs to be set. If we decide to check all possible combinations containing from 1 to n elements, this will lead us to $2^p$ combinations to check, which is technically impossible, or will take absurd amount of time to deliver the result. In this work, the limitation for the number of variables will be set 1:10, which means for we will take one predictor from 10 possible companies. For example, for 68 companies we will take from 6 to 7 variables into the final regression and from 43 companies we will take 4 variables into the final regression.\\
However, we also won't check all the possible combinations for given number of variables. If we use this naive implementation, the program has to run calculations for all \[\binom{n}{k} = \frac{n!}{k!(n-k)!}\] possible combinations. This is also will take too long to deliver the result. Before the first-order cut-off procedure the number of the combinations will be $\frac{68!}{5!(68-5)!} = 10424128$. For every combination the corresponding data matrix will be computed and model.fit() function applied. Plus, if we need additional results like LLR-test, AIC or BIC, RÂ², etc. we will increase the run time for each combination. Currently, with multithreading (the process is broken into 8 bunches for 8 cores), one iteration takes 2,4 seconds. The whole run time is: \[\frac{2,4*10424128}{8*60*60*24} = 36.19 days\]
As you can see, this is an acceptable run-time for a such easy task.\\
In order to reduce the initial amount of possible companies we will run the first-order cut-off. This cut-off is nothing else, but leaving out the companies with insignificant correlation with the dependent variable. For this task the correlation vector $\rho_{(Intel, company_{i})} \thinspace \forall i\in I$ will be calculated and only companies with correlation coefficient above 30\% will be used further.\\ 
In order to choose the best combination we will use Forward selection. This is a step-wise approach where we estimate the model with smallest amount of variables, choosing the best one. At the second iteration the regression with the company from the first iteration and one company from the rest will be estimated and the best is taken for further iteration. As the criterion of goodness of fit we will use LLR-test (Likelihood ratio)
\begin{enumerate}
	\item The correlation vector is ranged in uprising order. And the company with the highest correlation coefficient is taken. For this company the LLR, p-value and $R^2$ are computed.
	\item The regression with two explanatory variables will e built: the company from the first iteration and the company from the rest. For this regression same tests and values will be calculated. Then the best variant among 2-variables-regression will be chosen and compared to the regression from the previous iteration.
	\item These iterations will be continued until the previous step yields better result or the limit on the number of the variables is reached. 
\end{enumerate} 
As the final result, the first algorithm will return the regression model in the following form: \[Dependent = \alpha + \sum_{i=0}^{N} \beta_{i}Comp_{i}\] 
=======
The goal of this bachelor work is to check the ability of sentimental data to fine-tune "bad" forecasting models. Thus we will take a rather simple model with low predicting power - linear regression. On one hand, the disadvantage of this class of models is non flexibility over time, no built-in psychological component and low predicting ability. On the other hand, the advantages are simple and clear structure of the model, the number of parameters we have to deal with is limited and the approaches for estimation are rather simple.\\
The result of the first algorithm is the "best match" multiple linear regression. The limited number of regressors is chosen among the regressors in the given data. \\
In order to find the best regression different approaches can be used. However, if we decide to check all possible combinations containing from 1 to n elements, this will lead us to $2^p$ combinations. Even the modern machines won't be able to calculate models for all the combinations in an acceptable time.\\
The second naive approach is to set the strict limit on the number of the explanatory variables. Then we need to compute "critical value" for each of the models and choose the best. If we set the number of the explanatory variables to 5, we will have $\frac{68!}{5!(68-5)!} = 10424128$ combinations. The general formula to compute the number of combinations is:
\[\binom{n}{k} = \frac{n!}{k!(n-k)!}\] 
The approximate run time for i7 second generation, pypy and multithreading (8 cores) is 30 days ($\frac{2,4*10424128}{8*60*60*24} = 36.19 days$). This is also unacceptably slow.\\
In order to reduce the run time of the program we need to build a cut-off. The first-order cut off is the correlation vector ($\rho_{(Intel, company_{i})} \thinspace \forall i\in I$). We leave out the companies with correlation coefficient less than 30\%. Next step is to set upper limit for the number of the regressors in the equation. For this bachelor it will be enough to take 1:10 ratio. Which means one predictor from 10 possible companies will be taken into final equation. For example, for 68 companies we will take from 6 to 7 variables into the final regression and from 43 companies we will take 4 variables into the final regression.\\
As we saw above checking all possible combinations takes too much time. Thus we will use Forward selection. This is a step-wise approach where the model with smallest amount of variables is estimated first. At the second iteration the regression with the company from the first iteration and one company from the rest will be estimated and the best is taken for further iteration. As the criterion of goodness of fit we will use LLR-test (Likelihood ratio test) [12].
\begin{enumerate}
	\item The correlation vector is ranged in uprising order. The company with the highest correlation coefficient is taken as the only one explanatory varibale in the model. For this model the parameters are estimated, LLR, p-value and $R^2$ are computed.
	\item The regression with two explanatory variables will be built: the company from the first iteration and the company from the rest. For this regression parameters, tests and values will be calculated. Then the best variant among 2-variables-regression will be chosen and compared to the regression from the previous iteration using the critical value. The null hypothesis is that the regression with higher number of the variables is better. We use normal version of the LLR test because the models in our case are nested.
	\item These iterations will be continued until the previous step yields better result or the limit on the number of the variables is reached. 
\end{enumerate} 
The "best match" regression will be returned in form of string:
 \[Dependant = \alpha + \sum_{i=0}^{N} \beta_{i}Comp_{i}\] 
>>>>>>> da985b6d1458272d79a6665a4045e22bdc36d415
where N - is the limit on the number of variables in the regression, $\alpha$, $\beta_{i}$ are the regression's parameters.


\newpage
\section{conclusion}
\subsection{Interpretation of the results}
Here the difference of the actual value (test-sample) and our forecasts will be discussed and explained.
\subsection{Problems}
Here the biggest difficulties will be stated and also discussed the influence of the strong limitations.
\subsection{Potential}
Here most possible use and advantage of the model will be discussed.
\subsection{Improvements}
In order to be more useful the program can take several improvements...\\
Theoretical\\
Data\\
Technical

\newpage
\section{References}
\begin{enumerate}
	\item Wikipedia: linear regeression \url{http://en.wikipedia.org/wiki/Linear_regression}
\end{enumerate}

\end{document}